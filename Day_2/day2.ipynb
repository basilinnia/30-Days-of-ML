{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation <br>\n",
    "Measuring to quality of our model. There are many metrics for summarizing model quality, but we'll start with one called **Mean Absolute Error** (also called MAE). <br>\n",
    "\n",
    " Mean Absolute Error (MAE) is a measure of the average size of the mistakes in a collection of predictions, without taking their direction into account. It is measured as the average absolute difference between the predicted values and the actual values and is used to assess the effectiveness of a regression model. <br>\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"500\" height=\"200\" src=\"mae_.png\" alt=\"Material Bread logo\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(random_state=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(random_state=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(random_state=5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "melb_data = pd.read_csv(\".\\melb_data.csv\")\n",
    "filtered_data = melb_data.dropna(axis=0)\n",
    "y = filtered_data.Price\n",
    "melb_features = ['Rooms', 'Bathroom', 'Landsize','BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "X = filtered_data[melb_features]\n",
    "\n",
    "melbourne_model = DecisionTreeRegressor(random_state= 5)\n",
    "melbourne_model.fit(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we do calculate the mean absolute error: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434.71594577146544"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predicted_price = melbourne_model.predict(X)\n",
    "mean_absolute_error(y, predicted_price)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Data <br>\n",
    "Well we need more data to ensure our data is doing great so when we build a model, we want to make sure that it can generalize well to new and unseen data. To test how well our model will perform on new data, we need to evaluate it on data that was not used during the model-building process. This is where the validation data comes in. There can be some patterns in the given data, so if you give new/unseen data it learns something new.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_test_split splits the data into two separate subsets: one for training the model and the other for evaluating its performance\n",
    "\n",
    "train_X and train_y represent the features and target variable of the training data, respectively. The machine learning model will use this data to learn the relationship between the input features and the output variable.\n",
    "\n",
    "val_X and val_y represent the features and target variable of the validation data, respectively. This data is used to evaluate the performance of the machine learning model on new data that it hasn't seen during training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img width=\"300\" height=\"200\" src=\"train-test-split.png\" alt=\"Material Bread logo\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247256.56681730147\n"
     ]
    }
   ],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=3)\n",
    "\n",
    "melbourne_model = DecisionTreeRegressor()\n",
    "melbourne_model.fit(train_X, train_y)\n",
    "\n",
    "val_predictions = melbourne_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, val_predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underfitting and Overfitting\n",
    "Underfitting occurs when your model is too simple for your data. Overfitting occurs when your model is too complex for your data <br>\n",
    "<p align=\"center\">\n",
    "    <img src=\"over-underfit.png\" alt=\"Material Bread logo\">\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance and bias are two important concepts in statistics and machine learning that are related to the accuracy and generalization ability of a model.\n",
    "\n",
    "Bias refers to the systematic error that a model makes in its predictions. It can be thought of as the difference between the expected prediction of the model and the true value of the target variable. Models with high bias tend to oversimplify the problem and underfit the data, leading to poor performance on both the training and test sets.\n",
    "\n",
    "Variance, on the other hand, refers to the variability of the model's predictions for different training sets. It measures how sensitive the model is to small fluctuations in the training data. Models with high variance tend to overfit the training data and perform poorly on new, unseen data.\n",
    "\n",
    "In summary, bias refers to the error due to a model's simplifying assumptions, while variance refers to the error due to the model's sensitivity to fluctuations in the training data. A good model should balance both bias and variance to achieve high accuracy and good generalization ability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](mae_over.png)\n",
    "![image](bias.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return (mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a decision tree, each node represents a decision based on the value of a particular feature or attribute. The maximum number of leaf nodes in a decision tree refers to the maximum number of final decision nodes (i.e., leaves) that can be present in the tree. <br> <br>\n",
    "\n",
    "A decision tree algorithm typically tries to create a tree with a large number of leaf nodes to capture the nuances of the data and make accurate predictions. However, having too many leaf nodes can lead to overfitting, where the tree is too complex and captures noise or random fluctuations in the data rather than meaningful patterns. <br> <br>\n",
    "\n",
    "Therefore, setting a maximum number of leaf nodes is a common way to prevent overfitting and create a simpler decision tree. This limit can be set by the user or specified as a hyperparameter to the algorithm. The decision tree algorithm will stop splitting the tree when the maximum number of leaf nodes is reached, even if the algorithm could have continued to create more splits and more leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max leaf nodes: 5  \t\t Mean Absolute Error:  341969\n",
      "Max leaf nodes: 25  \t\t Mean Absolute Error:  264313\n",
      "Max leaf nodes: 50  \t\t Mean Absolute Error:  247991\n",
      "Max leaf nodes: 250  \t\t Mean Absolute Error:  236233\n",
      "Max leaf nodes: 500  \t\t Mean Absolute Error:  236202\n",
      "Max leaf nodes: 2500  \t\t Mean Absolute Error:  249578\n",
      "Max leaf nodes: 5000  \t\t Mean Absolute Error:  251285\n"
     ]
    }
   ],
   "source": [
    "# compare MAE with differing values of max_leaf_nodes\n",
    "for max_leaf_nodes in [5, 25, 50, 250, 500, 2500, 5000]:\n",
    "    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n",
    "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
