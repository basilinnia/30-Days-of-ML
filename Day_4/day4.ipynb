{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1480000.0\n",
      "1    1035000.0\n",
      "2    1465000.0\n",
      "3     850000.0\n",
      "4    1600000.0\n",
      "Name: Price, dtype: float64\n",
      "       Suburb           Address  Rooms Type Method SellerG       Date   \n",
      "0  Abbotsford      85 Turner St      2    h      S  Biggin  3/12/2016  \\\n",
      "1  Abbotsford   25 Bloomburg St      2    h      S  Biggin  4/02/2016   \n",
      "2  Abbotsford      5 Charles St      3    h     SP  Biggin  4/03/2017   \n",
      "3  Abbotsford  40 Federation La      3    h     PI  Biggin  4/03/2017   \n",
      "4  Abbotsford       55a Park St      4    h     VB  Nelson  4/06/2016   \n",
      "\n",
      "   Distance  Postcode  Bedroom2  Bathroom  Car  Landsize  BuildingArea   \n",
      "0       2.5    3067.0       2.0       1.0  1.0     202.0           NaN  \\\n",
      "1       2.5    3067.0       2.0       1.0  0.0     156.0          79.0   \n",
      "2       2.5    3067.0       3.0       2.0  0.0     134.0         150.0   \n",
      "3       2.5    3067.0       3.0       2.0  1.0      94.0           NaN   \n",
      "4       2.5    3067.0       3.0       1.0  2.0     120.0         142.0   \n",
      "\n",
      "   YearBuilt CouncilArea  Lattitude  Longtitude             Regionname   \n",
      "0        NaN       Yarra   -37.7996    144.9984  Northern Metropolitan  \\\n",
      "1     1900.0       Yarra   -37.8079    144.9934  Northern Metropolitan   \n",
      "2     1900.0       Yarra   -37.8093    144.9944  Northern Metropolitan   \n",
      "3        NaN       Yarra   -37.7969    144.9969  Northern Metropolitan   \n",
      "4     2014.0       Yarra   -37.8072    144.9941  Northern Metropolitan   \n",
      "\n",
      "   Propertycount  \n",
      "0         4019.0  \n",
      "1         4019.0  \n",
      "2         4019.0  \n",
      "3         4019.0  \n",
      "4         4019.0  \n",
      "16\n",
      "Low Cardinality Columns:  ['Type', 'Method', 'Regionname'] [dtype('O'), dtype('O'), dtype('O')]\n",
      "numerical_cols:  ['Rooms', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude', 'Propertycount']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('./melb_data.csv')\n",
    "\n",
    "# Separate target from predictors\n",
    "y = data.Price\n",
    "print(y.head())\n",
    "X = data.drop(['Price'], axis=1)\n",
    "print(X.head())\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "# Drop columns with missing values (simplest approach)\n",
    "# inplace = If False, return a copy. Otherwise, do operation inplace and return None.\n",
    "cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()]\n",
    "X_train_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "# The function nunique counts the number of distinct elements in specified axis.\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == \"object\"]\n",
    "print(X_train_full.columns.nunique())\n",
    "print(\"Low Cardinality Columns: \", low_cardinality_cols, [X_train_full[cname].dtype for cname in X_train_full[low_cardinality_cols]])\n",
    "\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "print(\"numerical_cols: \", numerical_cols)\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type              True\n",
      "Method            True\n",
      "Regionname        True\n",
      "Rooms            False\n",
      "Distance         False\n",
      "Postcode         False\n",
      "Bedroom2         False\n",
      "Bathroom         False\n",
      "Landsize         False\n",
      "Lattitude        False\n",
      "Longtitude       False\n",
      "Propertycount    False\n",
      "dtype: bool\n",
      "Type          True\n",
      "Method        True\n",
      "Regionname    True\n",
      "dtype: bool\n",
      "Categorical variables:\n",
      "['Type', 'Method', 'Regionname']\n"
     ]
    }
   ],
   "source": [
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "print(s)\n",
    "print(s[s])\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables\n",
    "There are 3 approaches while categorizing the data. These are: \n",
    "1. Drop Categorical Variables -> simply removing them from the dataset\n",
    "2. Ordinal Encoding -> assigning each unique value to a different integer\n",
    "3. One-Hot Encoding -> creating new columns indicating the presence (or absence) of each possible value in the original data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Categorical Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_X_train = X_train.select_dtypes(exclude=[\"object\"])\n",
    "dropped_X_valid = X_valid.select_dtypes(exclude=[\"object\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Make copy to avoid changing original data\n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "ord_encoder = OrdinalEncoder()\n",
    "label_X_train[object_cols] = ord_encoder.fit_transform(X_train[object_cols])\n",
    "label_X_valid[object_cols] = ord_encoder.transform(X_valid[object_cols])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We set handle_unknown='ignore' to avoid errors when the validation data contains classes that aren't<br><br>\n",
    "represented in the training data, and setting sparse=False ensures that the encoded columns are <br><br>\n",
    "returned as a numpy array (instead of a sparse matrix). <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# .concat = Concatenate pandas objects along a particular axis\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "# Ensure all columns have string type\n",
    "OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "OH_X_valid.columns = OH_X_valid.columns.astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which approach is best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop categorical variables):\n",
      "175703.48185157913\n",
      "MAE from Approach 2 (Ordinal Encoding):\n",
      "165936.40548390493\n",
      "MAE from Approach 3 (One-Hot Encoding):\n",
      "166089.4893009678\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset(dropped_X_train, dropped_X_valid, y_train, y_valid))\n",
    "print(\"MAE from Approach 2 (Ordinal Encoding):\")\n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\")\n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can check the files day4_1 and day4_2 to learn why we need this categorization strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
